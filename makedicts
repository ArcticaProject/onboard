#!/bin/bash
#
# Script to create dictionaries for onboards word prediction.
#
# The languages for which dictionaries are built is determined
# by the installed languages for the aspell spell checker.
# In order to get more or different dictionaries, install
# the appropriate language packages for aspell and rerun this
# script. Aspell is only needed for dictionary creation, not
# when running onboard.
#
# Running the script the first time creates and populates
# two directories:
#
# ./training      - Contains training texts by language.
#                   It is only used during dictionary-creation and
#                   if non-existant or deleted its content
#                   will be recreated.
#                   Training texts are mainly downloaded from
#                   Project Gutenberg, http://www.gutenberg.org.
#
# ./dictionaries  - Holds system dictionaries in utf-8 encoding
#
#

DICTDIR="dictionaries"
DICTEXT="dict"
#DICTDIR="/tmp"
TRAININGDIR="training"
LANGUAGES=$(aspell dump dicts| grep -v "-")
INCLUDE_INFREQUENT=0
EXPAND_AFFIXES=1
VERBOSE=0

get_gutenberg()
{
    ext="txt"
    if [ ! -e "$2.$ext" ]; then
        url="http://www.gutenberg.org$1"
        fname="$2.$ext"
        echo "Downloading '$url' -> '$fname'"
        wget -O- -nv http://www.gutenberg.org$1 | gzip -d >"$fname"
    fi
}

# Create training directory and populate it with free books.
# At the moment they are only used to count word frequencies.
# Feel free to add more texts and languages.
get_training_texts()
{
    pushd ${TRAININGDIR} >/dev/null

    for lang in $LANGUAGES; do

        [ -d "$lang" ] || mkdir $lang
        pushd $lang >/dev/null

        case "$lang" in
            en)
                get_gutenberg /files/2600/2600.zip "War and Peace"
                get_gutenberg /files/28885/28885-8.zip "Alice's Adventures in Wonderland"
                get_gutenberg /files/20417/20417-8.zip "The Outline of Science, Vol. 1"
                ;;
            en_GB)
                get_gutenberg /files/1342/1342.zip "Pride and Prejudice"
                get_gutenberg /files/345/345.zip "Dracula"
                get_gutenberg /dirs/etext99/advsh12.zip "The Adventures of Sherlock Holmes"
                ;;
            en_US)
                get_gutenberg /files/74/74.zip "The Adventures of Tom Sawyer"
                get_gutenberg /files/833/833-8.zip "Theory of the Leisure Class"
                ;;
            es)
                get_gutenberg /files/28592/28592-8.zip "El pecado y la noche"
                get_gutenberg /files/12848/12848-8.zip "Las inquietudes de Shanti Andia"
                get_gutenberg /files/20401/20401-8.zip "Viage al Rio de La Plata y Paraguay"
                ;;
            de)
                get_gutenberg /files/19530/19530-8.zip "Strix - Die Geschichte eines Uhus"
                get_gutenberg /files/29957/29957-8.zip "Ein Stück Lebensgeschichte und andere Erzählungen"
                get_gutenberg /files/16264/16264-8.zip "Deutsches Leben der Gegenwart"
                ;;
            it)
                get_gutenberg /files/28869/28869-8.zip "Novelle e riviste drammatiche"
                get_gutenberg /files/29955/29955-8.zip "Passeggiate per l'Italia, vol. 1"
                get_gutenberg /files/17909/17909-8.zip "Le Amanti"
                ;;
            fr)
                get_gutenberg /files/18244/18244-8.zip "Contes merveilleux, Tome I"
                get_gutenberg /files/18962/18962-8.zip "Le dangereux jeune homme"
                get_gutenberg /files/12251/12251-8.zip "Contes à mes petites amies"
                ;;
            *)
                if [ ${#lang} == 2 ]; then
                    echo "No training texts specified for '$lang'."
                else
                    echo "No training texts specified for '$lang', but using '${lang::2}' anyway."
                fi
                ;;
        esac

        popd >/dev/null
    done

    popd >/dev/null
}

help()
{
cat >&2 << END
Usage: `basename $0` [-e|E] [-i|I] [-v] [languages...]
Script to create dictionaries for onboards word prediction.
Options:
 -e  Expand aspell affixes.
 -E  Don't expand aspell affixes
     Default: -e, Expand affixes
     Expanding affixes can dramatically increase the number of
     infrequent words for affix heavy languages like Italian.
     Consider using -I with -e.

 -i  Include infrequent words
 -I  Don't include infrequent words
     Default: -I, Don't include infrequent words
     Infrequent words are considered to be words known to aspell that
     don't exist in the training texts. Including infrequent words
     considerably increases the dictionary size for all languages.

 -v  More verbose output

 -h  Show this help.
END
}

cleanup()
{
    rm $KNOWNGOOD $TRAININGSET $TMPFILE
}
trap 'cleanup; exit 1' INT TERM


# process command line arguments
while getopts "eEiIvh" opt; do
	case "$opt" in
	e)
		EXPAND_AFFIXES=1
		;;
	E)
		EXPAND_AFFIXES=0
		;;
	i)
		INCLUDE_INFREQUENT=1
		;;
	I)
		INCLUDE_INFREQUENT=0
		;;
	v)
		VERBOSE=1
		;;
	\?|*)
		help
		exit 1
		;;
	esac
done
shift $(($OPTIND - 1))


# get languages from positional parameters
[ $# -gt 0 ] && LANGUAGES="$*"

# make sure the output directory exists
[ -d "$DICTDIR" ] || mkdir $DICTDIR

# get training texts
[ -d "$TRAININGDIR" ] || mkdir $TRAININGDIR
get_training_texts

# create dictionaries
for lang in $LANGUAGES; do
    DICTFILE="$DICTDIR/$lang.$DICTEXT"
    KNOWNGOOD=$(tempfile)
    TRAININGSET=$(tempfile)
    TMPFILE=$(tempfile)

    echo "Creating '$DICTFILE'..."

    # Use aspell as a source of words that are known to be correctly spelled.
    # Only words from this set are considered for the final dictionary.
	if [ "$EXPAND_AFFIXES" -eq "1" ]; then
	    # grep -v "'"     - reduce size of italian dict
	    # grep -ve "^.$"  - exclude single letter words - don't
	    aspell --lang=$lang dump master | aspell --lang=${lang::2} expand \
        | tr -s '[:blank:]' '\n' | grep -v "'" | sort >$KNOWNGOOD
    else
	    aspell --lang=$lang dump master | cut -d/ -f1 \
        | tr -s '[:blank:]' '\n' | sort >$KNOWNGOOD
    fi

    if [ -s "$KNOWNGOOD" ]; then  # does aspell know the language?

        # find training texts
        if [ ${#lang} == 2 ]; then
            LANGDIRS="$TRAININGDIR/${lang}*"
        else
            LANGDIRS="$TRAININGDIR/${lang::2} $TRAININGDIR/${lang}"
        fi
        echo "  Looking for training texts in:" $LANGDIRS
        TRAININGFILES=$(find ${LANGDIRS} -name "*.txt" 2>/dev/null)

        # Create a dump of words from the training data
        # sorted, one word per line
        :>$TMPFILE
        ifs=$IFS
        IFS=$'\n'
        for f in $TRAININGFILES; do
            [ "$VERBOSE" -eq "1" ] && echo "  processing '$f'"

            # warn if the english license header/footer cannot be stripped away
            header="\*\*\* \?START OF \(THIS\|THE\) PROJECT GUTENBERG"
            footer="\*\*\* \?END OF \(THIS\|THE\) PROJECT GUTENBERG"
            grep -G "$header" "$f" &>/dev/null \
            || echo "Warning: License header not found, not stripping header for '$f'."
            grep -G "$footer" "$f" &>/dev/null \
            || echo "Warning: License footer not found, not stripping footer for '$f'."

            # strip header/footer and extract words
            cat "$f" \
            | sed "1,/${header}/d" \
            | sed "/${footer}/,\$d" \
            | tr -c '[:alnum:]' ' '| tr -s '[:blank:]' '\n' \
            >>$TMPFILE
        done
        IFS=$ifs
        sort $TMPFILE >>$TRAININGSET
        echo "  $(cat $TRAININGSET | wc -l) total words in training set."

        # Count word frequencies and write the tuple (word, weight)
        # to the dictionary.
        uniq -c $TRAININGSET | grep -Fw -f $KNOWNGOOD \
        | awk "{ print \$2 \",\"\$1; }" >$TMPFILE

        if [ "$INCLUDE_INFREQUENT" -eq "1" ]; then
            # Add all words not in the training set to the dictionary.
            cat $KNOWNGOOD | grep -Fwv -f $TRAININGSET \
            | awk '{ print $0 ",0";  }' >>$TMPFILE
        fi

        # create dictionary, sort by weight (not required)
        sort -grs -t"," -k2 $TMPFILE >$DICTFILE
        echo -n "  Dictionary has $(cat $DICTFILE | wc -l) unique words, "
        echo "$(grep -vc ',0' $DICTFILE) with nonzero frequency."
    fi

    cleanup
done


