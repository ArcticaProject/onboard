#!/usr/bin/env python

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
# Author: marmuta <marmvta@gmail.com>
#

import sys
import pypredict
from pypredict import *


def main():
    model = pypredict.DynamicModel()

    if 1:
        with timeit("load"):
            model.load(sys.argv[1])
        context = [unicode(w) for w in sys.argv[2:] or [u""]]
    else:
        # debug
        tokens = tokenize_text(u"<s> Mary has a little lamb.")
        tokens = tokenize_text(u"Mary has a little lamb little.")
        tokens = tokenize_text(read_corpus("../../moby.txt"))
        model.learn_tokens(tokens)
        # fixme, remove <unk> <unk> <weird word> ngrams
        context = [u"xxxxx", u""]
        context = [u"import", u"pypredict", u""]

    with timeit("predict (50)"):
        choices = model.predictp(context, 50)

    with timeit("predict (all)"):
        choices = model.predictp(context=context, limit=-1,
                                 filter=False,     # don't filter control words
                                 normalize=False)  # no explicit normalization
    print_choices(model, context, choices)
    print model.memory_size(), sum(model.memory_size())


def print_choices(model, context, choices):
    n   = min(model.order, len(context))
    history = context[-n:-1]
    prefix  = context[-1]

    print
    print "history:", history, "prefix '%s' " % prefix

    psum = 0
    counts = []
    for x in choices:
        ngram = history + [x[0]]
        psum += x[1]
        padding = max(model.order-len(context),0)
        ng = [u""]*padding + ngram
        counts.append([model.get_ngram_count(ng[i:]) for i in range(model.order)])

    print "Probability sum %f for %d results" % (psum,len(choices))   # ought to be 1.0 for the whole vocabulary
    print "Words with zero probability: ", sum(1 for x in choices if x[1] == 0)
    for i,x in enumerate(choices[:20]):
        print "%10f " % x[1] + "".join("%8d " % c for c in counts[i]) + "'%s'" % x[0]

if __name__ == '__main__':
    main()


