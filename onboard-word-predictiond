#!/usr/bin/python3
# -*- coding: utf-8 -*-
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
# Author: marmuta <marmvta@gmail.com>
#

from __future__ import division, print_function, unicode_literals

import os
import time
import re
import threading
try:
    import queue
except:
    import Queue  # python2

from gi.repository import GObject
import dbus
import dbus.service
import dbus.mainloop.glib

import sys
import Onboard.pypredict as pypredict

#-------------------------------------------------------------------------
# Config - gconf and configuration stuff
#-------------------------------------------------------------------------

import sys
from optparse import OptionParser

INSTALL_DIR        = "/usr/share/onboard/"
LOCAL_INSTALL_DIR  = "/usr/local/share/onboard/"
USER_DIR           = ".onboard"

class Config (object):
    """
    Singleton Class to encapsulate the gconf stuff and check values.
    """

    def __new__(cls, *args, **kwargs):
        """
        Singleton magic.
        """
        if not hasattr(cls, "self"):
            cls.self = object.__new__(cls)
            cls.self._init()
        return cls.self

    def _init(self):
        """
        Singleton constructor, should only run once.
        """
        parser = OptionParser()
        parser.add_option("-d", "--debug", type="str", dest="debug",
                  help="DEBUG={notset|debug|info|warning|error|critical}")
        parser.add_option("-g", "--log-learning",
                  action="store_true", dest="log_learn", default=False,
                  help="log all learned text; off by default")
        options = parser.parse_args()[0]

        if options.debug:
            logging.basicConfig(level=getattr(logging, options.debug.upper()))
        else:
            logging.basicConfig()

        self.log_learn = options.log_learn

    def get_user_model_dir(self):
        return os.path.join(self.get_user_dir(), "models")

    def get_user_dir(self):
        return os.path.join(os.path.expanduser("~"), USER_DIR) 

    def get_system_model_dir(self):
        return os.path.join(self.get_install_dir(), "models")
        
    def get_install_dir(self):
        install_dir = os.path.dirname(os.path.abspath(__file__))

        # running from the source directory?
        if os.path.isfile(os.path.join( \
           install_dir, "data", "org.onboard-word-prediction.service")):
            result = install_dir
            
        # when installed to /usr/local
        elif os.path.isdir(LOCAL_INSTALL_DIR):
            result = LOCAL_INSTALL_DIR
            
        # when installed to /usr
        elif os.path.isdir(INSTALL_DIR):
            result = INSTALL_DIR
           
        return result


#-------------------------------------------------------------------------
# WordPredictor - dbus object
#-------------------------------------------------------------------------

class WordPredictor(dbus.service.Object):

    @dbus.service.method("org.onboard.PredictionInterface",
                         in_signature='assi', out_signature='as')
    def predict(self, lmdesc, context_line, limit):
        context = pypredict.tokenize_context(context_line)
        choices = self.get_prediction(lmdesc, context, limit)
        _logger.info("context=" + repr(context))
        _logger.info("choices=" + repr(choices[:5]))
        return [x[0] for x in choices]

    @dbus.service.method("org.onboard.PredictionInterface",
                         in_signature='assi', out_signature='a(sd)')
    def predictp(self, lmdesc, context_line, limit):
        context = pypredict.tokenize_context(context_line)
        choices = self.get_prediction(lmdesc, context, limit)
        _logger.info("context=" + repr(context))
        _logger.info("choices=" + repr(choices[:5]))
        return choices

    @dbus.service.method("org.onboard.PredictionInterface",
                         in_signature='assb', out_signature='as')
    def learn_text(self, lmids, text, allow_new_words):
        tokens, spans = pypredict.tokenize_text(text)
        models = get_models(lmids)
        for model in models:
            model.learn_tokens(tokens, allow_new_words)
            model.modified = True
        _logger.info("learn_text: tokens=" + repr(tokens[:10]))

        # debug: save all learned text for later optimization testing
        if config.log_learn:
            fn = os.path.join(config.get_user_dir(), "learned_text.txt")
            with open(fn, "a") as f:
                f.write(text + "\n")

        return tokens

    @dbus.service.method("org.onboard.PredictionInterface",
                         in_signature='asas', out_signature='(a(iis)aai)')
    def lookup_text(self, lmids, text):
        """
        Split <text> into tokens and lookup the individual tokens in each
        of the given language models. This method is meant to be a basis for
        highlighting (partially) unknown words in a display for recently
        typed text.

        The return value is a tuple of two arrays. First an array of tuples
        (start, end, token), one per token, with start and end index pointing
        into <text> and second a two dimensional array of lookup results.
        There is one lookup result per token and language model. Each lookup
        result is either 0 for no match, 1 for an exact match or -n for
        count n partial (prefix) matches.
        """

        toks, spans = pypredict.tokenize_sentence(text)
        tokens  = [(spans[i][0], spans[i][1], t) for i,t in enumerate(toks)]
        results = [[0 for lmid in lmids] for t in tokens]
        for i,lmid in enumerate(lmids):
            model = get_model(lmid)
            if model:
                for j,t in enumerate(tokens):
                    results[j][i] = model.lookup_word(t[2])

        _logger.info("lookup_words: tokens=%s results=%s" % \
                     (repr(tokens), repr(results)) )
        return tokens, results

    @dbus.service.method("org.onboard.PredictionInterface",
                         in_signature='s', out_signature='asa(ii)')
    def tokenize_text(self, text):
        tokens, spans = pypredict.tokenize_text(text)
        _logger.info("tokenize_text: tokens=" + repr(tokens[:10]))
        return tokens, spans

    @dbus.service.method("org.onboard.PredictionInterface",
                         in_signature='s', out_signature='as')
    def tokenize_context(self, text):
        tokens = pypredict.tokenize_context(text)
        _logger.info("tokenize_context: tokens=" + repr(tokens[:10]))
        return tokens

    @dbus.service.method("org.onboard.PredictionInterface",
                         in_signature='', out_signature='')
    def Exit(self):
        _logger.info("exiting")
        mainloop.quit()


    def get_prediction(self, lmdesc, context, limit):
        lmids, weights = parse_lmdesc(lmdesc)
        models = get_models(lmids)

        for m in models:
            # Kneser-ney perfomes best in entropy and ksr measures, but
            # failed in practice for anything but natural language, e.g.
            # shell commands.
            # -> use the second best available: absolute discounting
            #m.smoothing = "kneser-ney"
            m.smoothing = "abs-disc"

            # setup recency caching
            if hasattr(m, "recency_ratio"):
                # Values found with
                # pypredict/optimize caching models/en.lm learned_text.txt
                # based on multilingual text actually typed (--log-learning)
                # with onboard over ~3 months.
                # How valid those settings are under different conditions
                # remains to be seen, but for now this is the best I can do.
                m.recency_ratio = 0.811
                m.recency_halflife = 96
                m.recency_smoothing = "jelinek-mercer"
                m.recency_lambdas = [0.404, 0.831, 0.444]

        model = pypredict.overlay(models)
        #model = pypredict.linint(models, weights)
        #model = pypredict.loglinint(models, weights)

        choices = model.predictp(context, limit)

        return choices

#-------------------------------------------------------------------------
# language model handling
#-------------------------------------------------------------------------

def get_models(lmids):
    models = []
    for lmid in lmids:
        model = get_model(lmid)
        if model:
            models.append(model)
    return models

def get_model(lmid):
    """ get language model from cache or load it from disk"""
    lmid = canonicalize_lmid(lmid)
    if lmid in language_models:
        model = language_models[lmid]
    else:
        model = load_model(lmid)
        if model:
            language_models[lmid] = model
    return model

def parse_lmdesc(lmdesc):
    """
        extract language model ids and interpolation weights from
        the language model description.
    """
    lmids = []
    weights = []

    for entry in lmdesc:
        fields = entry.split(",")

        lmids.append(fields[0])

        weight = 1.0
        if len(fields) >= 2: # weight is optional
            try:
                weight = float(fields[1])
            except:
                pass
        weights.append(weight)

    return lmids, weights

def canonicalize_lmid(lmid):
    """
        Fully qualifies and unifies language model ids.
        Fills in missing fields with default values.
        The result is of the format "type:class:name".
    """
    # default values
    result = ["lm", "system", "en"]
    for i, field in enumerate(lmid.split(":")[:3]):
        result[i] = field
    return ":".join(result)

def load_model(lmid):
    type_, class_, name  = lmid.split(":")

    if type_ == "lm":
        if   class_ == "system":
            model = pypredict.DynamicModel()
            #return None
        elif class_ == "user":
            model = pypredict.CachedDynamicModel()
        else:
            _logger.error("unknown class component '%s' in lmid '%s'" % \
                          (class_, lmid))
            return None
    else:
        _logger.error("unknown type component '%s' in lmid '%s'" % \
                      (type_, lmid))
        return None

    filename = get_filename(lmid)
    do_load_model(model, filename)
    #load_queue.put((model, filename))

    return model

def do_load_model(model, filename):
    _logger.info("loading %s" % filename)
    try:
        model.modified = False
        model.load(filename)
    except IOError as e:
        pass
        _logger.warning("Failed to load language model '%s': %s (%d)" %
                        (filename, os.strerror(e.errno), e.errno))

def save_models():
    for lmid,model in list(language_models.items()):
        if can_save(lmid):
            save_model(model, lmid)

def can_save(lmid):
    type_, class_, name  = lmid.split(":")
    return class_ == "user"

def save_model(model, lmid):
    type_, class_, name  = lmid.split(":")
    filename = get_filename(lmid)

    if model.modified or \
       not os.path.exists(filename):
        _logger.info("saving language model '%s'" % filename)
        try:
            # create the path
            path = os.path.dirname(filename)
            if not os.path.exists(path):
                os.makedirs(path)

            if 1:
                # save to temp file
                basename, ext = os.path.splitext(filename)
                tempfile = basename + ".tmp"
                model.save(tempfile)

                # rename to final file
                if os.path.exists(filename):
                    os.rename(filename, filename + ".bak")
                os.rename(tempfile, filename)

            model.modified = False
        except (IOError, OSError) as e:
            _logger.warning("Failed to save language model '%s': %s (%d)" %
                            (filename, os.strerror(e.errno), e.errno))

def get_filename(lmid):
    type_, class_, name  = lmid.split(":")
    if class_ == "system":
        path = config.get_system_model_dir()
    else: # class_ == "user":
        path = config.get_user_model_dir()
    ext = type_
    return os.path.join(path, name + "." + ext)


#-------------------------------------------------------------------------
# auto save timer
#-------------------------------------------------------------------------

auto_save_interval = 10 * 60  # in seconds, 0=off
last_auto_save_time = 0

def auto_save_callback():
    global last_auto_save_time

    if auto_save_interval:   # 0=no auto save
        t = time.time()
        if t - last_auto_save_time > auto_save_interval:
            last_auto_save_time = t
            save_models()
    return True # run again

class LoadThread(threading.Thread):
    def run ( self ):
        while True:
            model = None
            filename = None
            model, filename = load_queue.get()

            if model:
                do_load_model(model, filename)
            load_queue.task_done()

#-------------------------------------------------------------------------
# main
#-------------------------------------------------------------------------

### Logging ###
import logging
_logger = logging.getLogger("onboard-word-predictiond")
###############

### Config Singleton ###
config = Config()
########################


if __name__ == '__main__':

    #_logger.setLevel(logging.DEBUG)
    _logger.info("Onboard word prediction D-bus service")

    # cache of language models
    language_models = {}

    # D-Bus init
    dbus.mainloop.glib.DBusGMainLoop(set_as_default=True)
    session_bus = dbus.SessionBus()
    name = dbus.service.BusName("org.onboard.WordPrediction", session_bus)
    object = WordPredictor(session_bus, '/WordPredictor')

    # setup auto save timer
    auto_save_timer = GObject.timeout_add_seconds(5, auto_save_callback)

    # start thread for asynchronous loading
    load_queue = queue.Queue(0)
    thread = LoadThread()
    thread.setDaemon(True)  # don't deadlock on exit
    thread.start()

    # testing
    # Stopping into the debugger while in a D-Bus call locks up all gnome apps,
    # so do the debugging here. Temporariliy change the bus name above to really
    # stop all clients from meddling.
    if 0:
        choices = object.predictp(["lm:system:en",
                                   "lm:user:en",
                                  ], "Moby ", 10)
    #    model = object.get_model('en:user:lm')
    #    model.count_ngram([u"Moby", u"Duck"])
        object.learn_text(["lm:user:en",
                          ], "Moby Duck the whale", True)
        model = get_model("lm:user:en")
        for ng in model.iter_ngrams():
            print(ng)
        choices = object.predictp(["lm:system:en",
                                   "lm:user:en",
                                  ], "Moby ", 10)

        models = [get_model("lm:system:en"),
                  get_model("lm:user:en")]
        print("overlay:   ", pypredict.overlay(models).predictp(["Moby", ""], 10))
        print("linint:    ", pypredict.linint(models).predictp(["Moby", ""], 10))
        print("loglinint: ", pypredict.loglinint(models).predictp(["Moby", ""], 10))

    # main loop
    _logger.info("waiting for clients")
    mainloop = GObject.MainLoop()
    try:
        mainloop.run()
    except KeyboardInterrupt:
        pass

    # exit
    GObject.source_remove(auto_save_timer)
    save_models()

