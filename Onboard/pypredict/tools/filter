#!/usr/bin/python3
# -*- coding: utf-8 -*-
# Onboard is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 3 of the License, or
# (at your option) any later version.
#
# Onboard is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.
#
# Copyright Â© 2012, marmuta <marmvta@gmail.com>
#
# This file is part of pypredict, Onboard.
#

import os
import sys
import fnmatch
import subprocess
import re
from optparse import OptionParser
from collections import Counter

import sys
from os.path import dirname, abspath

from pypredict import *


def main():
    parser = OptionParser(usage="Usage: %prog [options] model_in model_out")
    parser.add_option("-l", "--language", type="str", dest="lang_id", default="",
              help="language id for the spell checker, e.g. en_US")
    parser.add_option("-r", "--regex", type="str", dest="regex", default="",
              help="regular expression filter")
    parser.add_option("-v", "--vocabulary", type="str", dest="vocabulary_file",
              help="list of words to consider during model creation")
    parser.add_option("-u", "--max-unigrams", type="int",
              dest="max_unigrams", default=0,
              help="prune n-grams with counts below or equal the one of the "
                   "least frequent of the top max_unigrams unigram;"
                   "default 0, disabled")
    parser.add_option("-p", "--prune_count", type="int",
              dest="prune_count", default=0,
              help="prune n-grams with counts below or equal <prune-count>")
    parser.add_option("-q", "--quiet",
              action="store_true", dest="quiet", default=False,
              help="only show the final summary")
    options, args = parser.parse_args()

    vocabulary = read_vocabulary(options.vocabulary_file) \
                 if options.vocabulary_file else None

    out = None if options.quiet else sys.stdout
    regex = options.regex
    max_unigrams = options.max_unigrams
    prune_count = options.prune_count
    model_in_filename = args[0]
    model_out_filename = args[1]
    lang_id = options.lang_id

    spell_checker = None
    if lang_id:
        spell_checker = SpellChecker()
        spell_checker.set_backend(0)
        if not spell_checker.set_dict_ids([lang_id]):
            print("No spell checker dictionary found for '{}'".format(lang_id))

    with timeit("loading " + model_in_filename, out):
        if read_order(model_in_filename) == 1:
            model = DynamicModel()
        else:
            model = UnigramModel()
        model.load(model_in_filename)

    if max_unigrams:
        with timeit("prune by max unigrams", out):
            cnt = Counter(tokens)
            most_common = cnt.most_common(max_unigrams)
            if most_common:
                min_token = most_common[-1]
                _prune_count = min_token[1]
                #print("pruning", min_token, prune_count)
                model = model.prune(_prune_count)

    if prune_count:
        with timeit("prune by count", out):
            model = model.prune(prune_count)

    if regex:
        with timeit("regex filter", out):
            model = regex_filter(model, regex)

    if spell_checker:
        model = spell_check_model(model, spell_checker, out)

    with timeit("saving " + model_out_filename, out):
        model.save(model_out_filename)

    print_stats(model)

def regex_filter(model, regex):
    pattern = re.compile(regex)
    out_model = model.__class__(model.order)

    for it in model.iter_ngrams():
        ngram = it[0]
        count = it[1]
        for token in ngram:
            if not pattern.search(token):
                out_model.count_ngram(ngram, count)
    return out_model

def spell_check_model(model, spell_checker, out):
    """ This really only works with unigrams at the moment. """
    out_model = model.__class__(model.order)

    ngrams = []
    counts = []
    with timeit("reading model", out):
        for it in model.iter_ngrams():
            ngrams.append(it[0])
            counts.append(it[1])


    with timeit("checking spelling of {} ngrams".format(len(ngrams)), out):
        step = 10000
        correct_words = set()
        for i in range(0, len(ngrams), step):
            tokens = [ngram[0] for ngram in ngrams[i:i+step]]
            print ("spell-checked {}/{} tokens, checking {} ...)" \
                   .format(i, len(ngrams), len(tokens)))
            correct_words.update(spell_checker.query(tokens))

    with timeit("creating filtered model", out):
        for i, ngram in enumerate(ngrams):
            if ngram[0] in correct_words:
                out_model.count_ngram(ngram, counts[i])

    return out_model

def print_stats(model):
    counts, totals = model.get_counts()
    print("calculating stats...")
    for i,c in enumerate(counts):
        sys.stdout.write("%d-grams: types %10d, occurences %10d\n" % \
              (i+1, counts[i], totals[i]))
    print(model.memory_size())


class SpellChecker:

    def __init__(self):
        self.dict_ids = []

    def set_backend(self, backend):
        pass

    def set_dict_ids(self, dict_ids):
        self.dict_ids = dict_ids
        return True

    def query(self, tokens):
        correct_words = []

        args = ["hunspell", "-G", "-i", "UTF-8"]
        if self.dict_ids:
            args += ["-d", ",".join(self.dict_ids)]

        p = None
        try:
            p = subprocess.Popen(args, stdin=subprocess.PIPE,
                                       stdout=subprocess.PIPE,
                                       close_fds=True)
        except OSError as e:
            _logger.error(_format("Failed to execute '{}', {}", \
                            " ".join(args), e))

        # Check if the process is still running, it might have
        # exited on start due to an unknown dictinary name.
        if p and p.poll() is None:
            for token in tokens:
                line = (token + "\n").encode("UTF-8")
                p.stdin.write(line)
            p.stdin.close()

            while True:
                line = p.stdout.readline().decode("UTF-8")
                if not line:
                    break
                token = line.strip()
                correct_words.append(token)

        if p:
            p.terminate()
            p.wait()

        return correct_words


if __name__ == '__main__':
    main()

