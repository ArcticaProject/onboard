#!/usr/bin/python3

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
# Author: marmuta <marmvta@gmail.com>
#

import sys
from pypredict import *


def main():
    model = CachedDynamicModel()

    if 0:
        with timeit("loading"):
            model.load(sys.argv[1])
        context = [str(w) for w in sys.argv[2:] or [""]]
        model.recency_ratio = 0
    else:

        # usage:
        # pypredict/predict dummy [history] prefix
        # e.g. pypredict/predict "" www test ""

        model = CachedDynamicModel()

        # debug
        training_text = """
            No, when I go to sea, I go as a simple sailor, right before the mast,
            plumb down into the forecastle, aloft there to the royal mast-head.
            True, they rather order me about some, and make me jump from spar to
            spar, like a grasshopper in a May meadow. And at first, this sort
            of thing is unpleasant enough. And more than all,
            if just previous to putting your hand into the tar-pot, you have been
            lording it as a country schoolmaster, making the tallest boys stand
            in awe of you. The transition is a keen one, I assure you, from a
            schoolmaster to a sailor, and requires a strong decoction of Seneca and
            the Stoics to enable you to grin and bear it. But even this wears off in
            time.
            www.test.com
            www.gnome.org
            """
        tokens, spans = tokenize_text(training_text)
        #tokens, spans = tokenize_text(u"<s> Mary has a little lamb.")
        #tokens, spans = tokenize_text(u"Mary has a little lamb little.")
        #tokens, spans = tokenize_text(read_corpus("../../moby.txt"))
        #tokens, spans = tokenize_text(read_corpus("/home/user/.gpredict/learned_text.txt"))
        model.learn_tokens(tokens)
        for ng in model.iter_ngrams():
            if "bzr" in ng[0]:
                print(ng)
        context = [str(w) for w in sys.argv[2:] or [""]]
        # fixme, remove <unk> <unk> <weird word> ngrams
        #context = [u"xxxxx", u""]
        #context = [u"import", u"pypredict", u""]
        model.recency_ratio = 1

    with timeit("predict (50)"):
        choices = model.predictp(context, 50)

    with timeit("predict (all)"):
        choices = model.predictp(context=context, limit=-1,
                                 filter=False,     # don't filter control words
                                 normalize=False)  # no explicit normalization
    print_choices(model, context, choices)
    print(model.memory_size(), sum(model.memory_size()))


def print_choices(model, context, choices):
    n   = min(model.order, len(context))
    history = context[-n:-1]
    prefix  = context[-1]

    print()
    print("history:", history, "prefix '%s' " % prefix)

    psum = 0
    counts = []
    for x in choices:
        ngram = history + [x[0]]
        psum += x[1]
        padding = max(model.order-len(context),0)
        ng = [""]*padding + ngram
        counts.append([model.get_ngram_count(ng[i:]) for i in range(model.order)])

    print("Probability sum %f for %d results" % (psum,len(choices)))   # ought to be 1.0 for the whole vocabulary
    print("Words with zero probability: ", sum(1 for x in choices if x[1] == 0))
    for i,x in enumerate(choices[:20]):
        print("%10f " % x[1] + "".join("%8d " % c for c in counts[i]) + "'%s'" % x[0])

if __name__ == '__main__':
    main()

