#!/usr/bin/env python
# -*- coding: latin-1 -*-

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
# Author: marmuta <marmvta@gmail.com>
#

import sys,math
import time
import codecs
import re
import gc
from contextlib import contextmanager
from numpy import *
import numpy as np

@contextmanager
def timeit():
    gc.collect()
    gc.collect()
    gc.collect()
    t = time.time()
    yield None
    print "time: %fms" % ((time.time() - t)*1000)


def main():
    global model # for debugging

    model = LanguageModel(max_order = int(sys.argv[2]), smoothing = int(sys.argv[3]))
    training, held_out, testing = model.read_corpus(sys.argv[1])
    context = sys.argv[4:] or [""]

    model.train(training)

    # incrementally add new ngrams
    #model.train(held_out)   # abuse held_out for now
    #model.train(testing)    # abuse testing for now
    #model.train(["Mary has a little lamb"])
    #model.train(["John read Moby Dick","Mary read a different book","She read a book by Cher"])
    #model.train(["a a b"])

    model.info()

    #model.test(testing)

    #choices = model.predict(["when","the", ""])
    #choices = model.predict(["Alice", "has", "a", "little", ""])

  #  with timeit():
  #      choices = model.predict(context)

  #  with timeit():
  #      choices2 = model.predict_kn_v(context)

    with timeit():
        choices3 = model.predict_kn_numpy(context)

  #  model.print_choices(context, choices)
  #  model.print_choices(context, choices2)
    model.print_choices(context, choices3)

    #time.sleep(1000)



def bisect_left(a, prefix, value_func=lambda x: x, lo=0, hi=None):
    """
        Binary search based on python's bisect.bisect_left.
        value_func returns the item to compare with prefix.
    """
    if lo < 0:
        raise ValueError('lo must be non-negative')
    if hi is None:
        hi = len(a)
    while lo < hi:
        mid = (lo+hi)//2
        if value_func(a[mid]) < prefix: lo = mid+1
        else: hi = mid
    return lo

def binsearch(a, key, value_func=lambda x: x, lo=0, hi=None):
    index = bisect_left(a, key, value_func, lo, hi)
    if index < len(a) and a[index] == key:
        return index
    return -1

class Vocabulary:
    def __init__(self):
        self.words = []
        self.sorted = []

    def add_word(self, word):
        wi = len(self.words)
        self.words.append(word)
        index = self.index(word)
        self.sorted.insert(index, wi)
        return wi

    def word_to_index(self, word):
        index = self.index(word)
        if index >= 0 and index < len(self.sorted):
            wi = self.sorted[index]
            if self.words[wi] == word:
                return wi
        return -1

    def words_to_indices(self, words):
        indices = []
        for word in words:
            indices.append(self.word_to_index(word))
        return indices

    def index_to_word(self, index):
        return self.words[index]

    def indices_to_words(self, indices):
        words = []
        for i in indices:
            words.append(self.words[i])
        return words

    def index(self, word):
        """ lookup word through sorted indices """
        return bisect_left(self.sorted, word, lambda x: self.words[x])

    def __getitem__(self, index):
        return self.words[self.sorted[index]]

    def __len__(self):
        assert(len(self.words) == len(self.sorted))
        return len(self.words)

class Trie:
    children = None
    wi     = -1
    count  = 0
    N1pxr  = 0   # number of word types wi-n+1 that precede wi-n+2..wi in the training data
    N1pxrx = 0   # number of permutations around center part

    def __init__(self, wi = -1):
        self.wi = wi

    def get_child(self, wi):
        if self.children:
            index = self.index(wi)
            #print wi,i ,self.children,wi < len(self.children)
            if index < len(self.children):
                if self.children[index].wi == wi:
                    return self.children[index]
        return None

    def add_child(self, wi):
        node = Trie(wi)
        if self.children:
            index = self.index(wi)
            self.children.insert(index, node)
        else:
            self.children = [node]
        return node

    def get_count(self, wis):
        node = self.get_node(wis)
        return node.count if node else 0

    def get_node(self, wis):
        node = self
        for wi in wis:
            node = node.get_child(wi)
            if not node:
                break
        return node

    def index(self, wi):
        return bisect_left(self.children, wi, lambda x: x.wi)

    def items(self, order):
        return [(ngram, count) for ngram, count in self.iter_ngrams() if len(ngram)-1 == order]

    def count_ngrams(self, max_order):
        counts = [0]*max_order
        totals = [0]*max_order
        for ngram, count in self.iter_ngrams():
            i = len(ngram)-1
            counts[i] += 1
            totals[i] += count
        return counts, totals

    def iter_ngrams(self):
        """ recursively traverse all children starting from parent "node" """
        for ngram, node in self.iter_nodes(self, []):
            yield ngram, node.count

    def iter_nodes(self, node=None, ngram=[]):
        """ recursively traverse all children starting from parent "node" """
        if not node:
            node = self
        if node != self:
            yield ngram, node
        if node.children:
            for child in node.children:
                for ng, nd in self.iter_nodes(child, ngram+[child.wi]):
                    yield ng, nd

    def iter_children(self, wis=None):
        """ direct children only, no recursion """
        if wis:
            node = self.get_node(wis)
        else:
            node = self
        if node:
            for child in node.children:
                yield child

    def __repr__(self):
        return  "wi=%d count=%d N1pxr=%d N1pxrx=%d children=%s " % \
                 (self.wi, self.count, self.N1pxr, self.N1pxrx, \
                 str(len(self.children)) if not self.children is None else "None")


class LanguageModel:

    MLE, LAPLACE, KATZ_BO, \
    WITTEN_BELL_I, ABS_DISC_I, \
    KNESER_NEY_I = range(6)

    smooth_name = ("Maximum Likelihood Estimate",
                   "Laplace",
                   "Katz back-off",
                   "Witten-Bell interpolated",
                   "Absolute Discounting interpolated",
                   "Kneser-Ney interpolated",
                   )

    def __init__(self, filename = None, max_order = 3, smoothing = MLE):
        self.filename = filename
        self.smoothing = smoothing
        self.set_order(max_order)

    def set_order(self, max_order):
        self.max_order = max_order
        self.reset()

    def reset(self):
        self.num_words = 0         # number of word types
        self.num_tokens = 0        # total words in training data

        r = range(self.max_order)

        self.vocabulary = Vocabulary()
        self.ngrams = Trie()

        self.n1s    = [0 for x in r]
        self.n2s    = [0 for x in r]
        self.Ds     = [0 for x in r]
        self.N1rxs  = [{} for x in r]
        self.N1xrs  = [{} for x in r]
        self.N1xrxs = [{} for x in r]

    def read_corpus(self, filename):
        # read corpus
        #s = codecs.open(filename, encoding='utf-8').read() \
        s = codecs.open(filename, encoding='latin-1').read() \
            .replace("\r"," ") # remove carriage returns from Moby Dick

        # split into sentences including separaters (punctuation, newlines)
        sentences = re.findall(""".*?(?:[\.;:!?][\s\"]  # punctuation
                                      | \s*\\n\s*\\n)   # double newline
                               """, s, re.UNICODE|re.DOTALL|re.VERBOSE)
        # devide corpus into 3 sections: training, held_out, test
        r = range(len(sentences))
        sh = set(r[5::20])
        st = set(r[15::20])
        training  = [sentences[i] for i in set(r) - sh - st]
        held_out  = [sentences[i] for i in sh]
        testing   = [sentences[i] for i in st]
        print "sentences: total %d, training %d, held_out %d, testing %d" % \
              (len(sentences),len(training),len(held_out),len(testing))

        return training, held_out, testing

    def train(self, sentences):
        assert(not self.ngrams is None)
        self.train_sentences(sentences)

    def info(self):
        counts,totals = self.ngrams.count_ngrams(self.max_order)
        for i,c in enumerate(counts):
            print "%d-grams: count %d, total %d" % \
                  (i+1, counts[i], totals[i])
        l = sum(len(w) for w in self.get_vocabulary()) * 1.0/ len(self.get_vocabulary())
        print "vocabulary: distinct words %d, total word tokens %d, average word length %f" % \
               (self.num_words, self.num_tokens, l)

    def test(self, sentences):
        print
        print "testing..."
        if 0:
            print self.split_sentence("aaa, bbb m-a drop-off gnome-volume-manager the--when, d,,h, x+-3 x+=3 yyy*=vv /= == <> <= >= &= != ^= mmm'mmm mmm''mmm")
            print self.split_sentence("1.0 234.0654 6.0.1 ee4.0 abc56.8.3 onboard_0.92.0-0ubuntu3 fff")
            print self.split_sentence("./onboard /home/ubuntu /home/ubuntu/test_dir.x/onboard_0.92.0-0ubuntu3.dsc ~/test")
            print self.split_sentence("x/60/60 x/60/float(60) ")
            print self.split_sentence("slashdot.org www.slashdot.org http://docs.python.org/library/re.html#re.UNICODE")
            print self.split_sentence("http://en.wikipedia.org/w/index.php?title=Special:Search&search=kneser-ney&fulltext=Search&ns0=1&redirs=0")

        # test
        # print n-grams
        if 0:
            for n in xrange(max_order):
                items = sorted(self.ngrams[n].items(), key=lambda x:(x[1], x[0]), reverse=True)
                for i,item in enumerate(items):
                    print "%d:" % i, item[1], item[0]

        if 0:
            for ngram,count in self.ngrams.iter_ngrams():
                print self.vocabulary.indices_to_words(ngram),count

        if 0:
            for ngram,node in self.ngrams.iter_nodes():
                print node,self.vocabulary.indices_to_words(ngram)

        # chech total number of n-grams with exactly one and two counts
        n1s = [0]*self.max_order
        n2s = [0]*self.max_order
        for ng,count in self.ngrams.iter_ngrams():
            i = len(ng)-1
            if count == 1: n1s[i] += 1
            if count == 2: n2s[i] += 1

        for i in range(self.max_order):
            print "%d: n1 %10d, n2 %10d, D: %10f" % (i+1,self.n1s[i],self.n2s[i],self.Ds[i])
            print "%d: n1 %10d, n2 %10d" % (i+1,n1s[i],n2s[i])
            assert(self.n1s[i] == n1s[i])
            assert(self.n2s[i] == n2s[i])



        for i in range(self.max_order):
            print "%d: left %10d, right %10d,  center %10d" % \
                   (i+1,len(self.N1rxs[i]),len(self.N1xrs[i]),len(self.N1xrxs[i]))

        # print bigram matrices for the top x words
        if self.max_order >= 2:
            ntop = 8
            items = sorted(self.ngrams.items(0), key=lambda x:(x[1], x[0]), reverse=True)[:ntop]
            top = [self.vocabulary.index_to_word(x[0][0]) for x in items]
            mc = [["%d" % self.get_ngram_count([y, x]) for x in top] for y in top]
            mp = [["%7f" % self.get_probability([y, x]) for x in top] for y in top]
            maxlen = max(len(x) for y in mc for x in y)
            maxlen = max(maxlen, max(len(x) for y in mp for x in y))
            print
            print "top %d bigram probability matrix:" % ntop
            self.print_matrix(top, mp, maxlen)
            print
            print "top %d bigram count matrix:" % ntop
            self.print_matrix(top, mc, maxlen)

        # test
        self.test_sentences(sentences)
        self.test_sentences(["I spent three years before the mast"])
        self.test_sentences(["John read a book"])

        # test normalization, all numbers should be close to 1.0
        for i in range(self.max_order):
            history = ["in", "the", "end"][:max(i,0)]
            history = ["<s>", "a", "a"][:max(i,0)]
            ps = 0
            for w in self.get_vocabulary():
                ps += self.get_probability(history + [w])
            print "order %d: %10f" % (i+1,ps),history


    def print_matrix(self, top, matrix, maxlen=0):
        maxlen = max(maxlen, max(len(x) for x in top))
        maxlen = max(maxlen, max(len(x) for y in matrix for x in y))
        print "".ljust(maxlen) + " " + " ".join(x.rjust(maxlen) for x in top)
        for i,y in enumerate(top):
            print y.ljust(maxlen) + " " + " ".join(x.rjust(maxlen) for x in matrix[i])

    def train_sentences(self, sentences):
        """ find and count n-grams """
        for sentence in sentences:
            parts,count = self.split_sentence(sentence)
            self.num_tokens += count

            # extract and count n-grams
            for i,part in enumerate(parts):
                for n in xrange(self.max_order):
                    end = i+n+1
                    if end <= len(parts):
                        assert(n == len(parts[i:i+n+1])-1)
                        self.add_ngram(parts[i:i+n+1])

        self.num_words  = len(self.vocabulary) # number of words in vocabulary
        self.finish_precalculations()
        #self.vocabulary_map = word_index_map(self.vocabulary)

    def ngram_to_indices(self, ngram):
        return tuple(self.vocabulary.words_to_indices(ngram))

    def add_ngram(self, ngram):
        i = len(ngram) - 1

        # add words to the vocabulary
        wis = []
        for w in ngram:
            wi = self.vocabulary.word_to_index(w)
            if wi == -1:
                wi = self.vocabulary.add_word(w)
            wis.append(wi)

        # get/add node for ngram
        node = self.ngrams
        for wi in wis:
            child = node.get_child(wi)
            if child == None:
                child = node.add_child(wi)
            node = child

        count = node.count

        # remove old state
        if count == 1:
            self.n1s[i] -= 1
        if count == 2:
            self.n2s[i] -= 1

        count += 1
        node.count = count

        # add new state
        if count == 1:
            self.n1s[i] += 1
        if count == 2:
            self.n2s[i] += 1


        if count == 1:    # only the first time for each ngram

            # get/add node for ngram excluding predecessor
            # Predecessors exist for unigrams or greater,
            # Use root for empty remainder, i.e. predecessor of nothing are all unigrams
            node = self.ngrams
            for wi in wis[1:]:
                child = node.get_child(wi)
                if child == None:
                    child = node.add_child(wi)
                node = child
            node.N1pxr += 1    # count number of word types wi-n+1 that precede wi-n+2..wi in the training data

            # get/add node for ngram excluding predecessor and successor
            # Predecessors and successors exist only for bigrams or greater.
            # Use root for empty remainder, i.e. nothing is surrounded by all bigrams.
            if len(wis) >= 2:
                node = self.ngrams
                for wi in wis[1:i]:
                    child = node.get_child(wi)
                    if child == None:
                        child = node.add_child(wi)
                    node = child
                node.N1pxrx += 1    # count number of permutations around center

#        # rx:  number of unique words that follow the history (left remainder)
#        # xr:  number of different words wi-n+1 that precede wi-n+2..wi in the training data
#        #      count the number of first words occuring for fixed top parts of the n-grams
#        # xrx: number of permutations around center
#        # x:   mark for loop through all words of the vocabulary
#        if count == 1:    # only the first time for each ngram
#            key = tuple(wis)
#            lr = key[:i] # [<left remainder>, x], ['the', 'room', x]
#            self.N1rxs[i][lr] = self.N1rxs[i].get(lr, 0) + 1

#            rr = key[1:] # [x, <right remainder>], [x, 'the', 'room']
#            self.N1xrs[i][rr] = self.N1xrs[i].get(rr, 0) + 1

#            cr = key[1:i] # [x, <center remainder>, x], [x, 'the', x]
#            self.N1xrxs[i][cr] = self.N1xrxs[i].get(cr, 0) + 1


    def finish_precalculations(self):
        for i in range(self.max_order):
            n1, n2 = self.n1s[i],self.n2s[i]
            if n1 or n2:
                D = n1 / (n1 + 2.0*n2) # deleted estimation, Ney, Essen, and Kneser 1994
            else:
                D = .1   # training corpus too small, fake a value
            assert(0 <= D and D <= 1.0)
            self.Ds[i] = D


    def test_sentences(self, sentences):
        word_count = 0
        ngram_count = 0
        entropy = 0
        for sentence in sentences:
            parts,count = self.split_sentence(sentence)
            word_count += count   # excluding sentence begin/end

            # extract n-grams of maximum length
            for i in xrange(len(parts)):
                b = max(i-(self.max_order-1),0)
                e = min(i-(self.max_order-1)+self.max_order,len(parts))
                ngram = parts[b:e]
                if len(ngram) != 1 or ngram[0] != "<s>":  # excluding unigram <s>
                    p = self.get_probability(ngram)
                    e = math.log(p,2) if p else float("infinity")
                    entropy += e
                    ngram_count += 1
                    #print p, e, ngram

        entropy = -entropy/word_count if word_count else 0
        try:
            perplexity = 2 ** entropy
        except:
            perplexity = 0
        print "test: words %d, n-grams %d, entropy %f bit/word, perplexity %f" % \
              (word_count, ngram_count, entropy, perplexity)

    def get_vocabulary(self):
        return self.vocabulary

    def split_sentence(self, sentence):
        # split into sentence parts (words, punctuation)
#        parts = re.findall(u"""  (?:http://)?[\w\-]+(?:\.[\w\-]+)+(?:/[~\w\-\.]+)*
#                                 (?:\#[\w\-\.]*)? (?:[\?&][\w\-]+=[\w\-:]*)*  # urls
#                               | (?:^|\s|\.|~)(?:/[\w\-\.]+)+           # file names
#                               | \w*[\-]?\d+(?:\.\d+)+(?:\-\w+)*   # version numbers
#                               | \d+(?:\.\d+)?                          # floats
#                               | (?:[\w]+(?:[-'][\w]+)*)                # words
#                               | \-\- | \+[\+\-] | [\+\-\*/=!<>&\^]= | <>
#                               | \"\" | \"\"\" | \'\' | \'\'\'
#                               | \(\) | \[[0-9]?\] | \{\} | $\{ | \):
#                               | [\.,;:!¡?¿=\"\'\-\+\*/<>\(\)\[\]\{\}$%&\#\|]""",
#                               sentence, re.UNICODE|re.VERBOSE)
#        parts = re.findall(u"(?:[\w]+(?:[-'][\w]+)*)",
#                           sentence, re.UNICODE|re.VERBOSE)
        parts = re.findall(u"[^\W\d]\w*(?:[-'][\w]+)*",
                           sentence, re.UNICODE|re.VERBOSE)
        if parts:
            parts = ["<s>"] + parts + ["<\s>"]
        count = len(parts)
        return parts, count

    def get_ngram_count(self, ngram):
        wis = self.vocabulary.words_to_indices(ngram)
        return self.ngrams.get_count(wis)

    def get_probability(self, ngram):
        if self.smoothing == self.MLE:
            n  = len(ngram)  # order of the ngram
            c  = self.get_ngram_count(ngram)
            cs = self.get_ngram_count(ngram[:-1]) if n >= 2 else self.num_tokens
            p  = c / float(cs) if c else 0
        elif self.smoothing == self.LAPLACE:
            n  = len(ngram)  # order of the ngram
            c  = self.get_ngram_count(ngram)
            cs = self.get_ngram_count(ngram[:-1]) if n >= 2 else self.num_tokens
            p  = (c + 1) / (float(cs) + self.num_words)
        elif self.smoothing == self.KATZ_BO:
            p = 0
        elif self.smoothing == self.WITTEN_BELL_I:
            p = self.witten_bell_i(ngram)
        elif self.smoothing == self.ABS_DISC_I:
            p = self.absolute_discounting_i(ngram)
        elif self.smoothing == self.KNESER_NEY_I:
            p = self.kneser_ney_i(ngram)

        #print cw, c, num_words, p, ngram
        return p

    def witten_bell_i(self, ngram):
        n = len(ngram)
        if n == 0:
            return 1.0/self.num_words  # uniform distribution for order 0
        history = ngram[:-1]
        N1rx = self.N1rxs[n-1].get(tuple(history), 0)
        c = self.get_ngram_count(ngram)
        if n == 1:
            cs = self.num_tokens
        else:
            cs = self.get_ngram_count(history) # sum(c(w)) = c(history)
        if cs:
            l1 = N1rx / (N1rx + float(cs)) # 1 - gamma
            pmle = c / float(cs) if c else 0
            return (1.0 - l1)*pmle + l1*self.witten_bell_i(ngram[1:])
        else:
            # history unknown - return (n-1)-gram result
            return self.witten_bell_i(ngram[1:])

    def absolute_discounting_i(self, ngram):
        n = len(ngram)
        if n == 0:
            return 1.0/self.num_words  # uniform distribution for order 0
        history = ngram[:-1]
        D = self.Ds[n-1]
        #D = .15
        N1hx = self.N1rxs[n-1].get(tuple(history), 0)
        c = self.get_ngram_count(ngram)
        if n == 1:
            cs = self.num_tokens
        else:
            cs = self.get_ngram_count(history) # sum_w(c(history+w)) = c(history)
        #print n,N1hx,c,cs,history,ngram
        if cs:
            l1 = D / float(cs) * N1hx   # 1 - gamma
            return max(c - D, 0) / float(cs) + l1 * self.absolute_discounting_i(ngram[1:])
        else:
            # history unknown - return (n-1)-gram result
            return self.absolute_discounting_i(ngram[1:])

    def kneser_ney_i(self, ngram):
        """ kneser_ney interpolated, recursive """
        n = len(ngram)
        if n == 0:
            return 1.0/self.num_words  # uniform distribution for order 0
        history = ngram[:-1]
        D = self.Ds[n-1]
        N1rx = self.N1rxs[n-1].get(self.ngram_to_indices(history), 0)
        a = 0
        l1 = 1  # if history is unknown return (n-1)-gram result
        if n < self.max_order:
            N1xr  = self.N1xrs[n].get(self.ngram_to_indices(ngram), 0)      # number of word types seen to precede ngram
            N1xrx = self.N1xrxs[n].get(self.ngram_to_indices(ngram[:n-1]), 0) # number of words preceding all words
            if N1xrx:
                a  = max(N1xr - D, 0) / float(N1xrx)  # alpha
                l1 = D / float(N1xrx) * N1rx       # 1 - gamma
        else:
            c = self.get_ngram_count(ngram)
            if n == 1:
                cs = self.num_tokens
            else:
                cs = self.get_ngram_count(history) # # sum_w(c(history+w)) = c(history)
            if cs:
                a  = max(c - D, 0) / float(cs)  # alpha
                l1 = D / float(cs) * N1rx       # 1 - gamma

        return a + l1 * self.kneser_ney_i(ngram[1:])

    def kneser_ney_i_i(self, ngram):
        """ kneser_ney interpolated, iterative """
        n = len(ngram)
        p = 1.0/self.num_words  # uniform distribution for order 0

        for i in range(0, n-1):
            ng = ngram[n-i-1:]
            N1xrx = self.N1xrxs[i+1].get(tuple(ng[:-1]), 0) # number of words preceding all words
            if N1xrx:
                D = self.Ds[i]
                N1xr  = self.N1xrs[i+1].get(tuple(ng), 0)        # number of word types seen to precede ngram
                N1rx  = self.N1rxs[i].get(tuple(ng[:-1]), 0)   # number of word types following the history
                a  = max(N1xr - D, 0) / float(N1xrx)  # alpha
                l1 = D / float(N1xrx) * N1rx       # 1 - gamma
                p = a + l1 * p

        history = ngram[:-1]
        if n == 1:
            cs = self.num_tokens
        else:
            cs = self.get_ngram_count(history) # sum_w(c(history+w)) = c(history)
        if cs:
            D = self.Ds[n-1]
            c = self.get_ngram_count(ngram)
            N1rx = self.N1rxs[n-1].get(tuple(history), 0)
            a  = max(c - D, 0) / float(cs)  # alpha
            l1 = D / float(cs) * N1rx       # 1 - gamma
            p = a + l1 * p

        return p

    def kneser_ney_i_v(self, history, words):
        """ kneser_ney interpolated, iterative, vectorized """

        n = len(history) + 1

        # order 0
        p0 = 1.0/self.num_words  # uniform distribution
        ps = [p0 for w in words]

        # order 1..n-1
        for j in range(0, n-1):
            h = tuple(history[n-j-1:])
            N1xrx = float(self.N1xrxs[j+1].get(h, 0)) # number of permutations around r
            if N1xrx:
                D = self.Ds[j]
                N1rx  = self.N1rxs[j].get(h, 0)   # number of word types following the history
                N1xrs = self.N1xrs[j+1]
                l1 = D / float(N1xrx) * N1rx     # 1 - gamma
                ngram = history[n-j-1:] + [None]

                for i in xrange(len(words)):
                    ngram[-1] = words[i]
                    N1xr = N1xrs.get(tuple(ngram), 0)        # number of word types seen to precede ngram
                    a = N1xr - D
                    if a < 0: a = 0
                    a /= N1xrx  # alpha
                    ps[i] = a + l1 * ps[i]

        # order n
        if n == 1:
            cs = float(self.num_tokens)
        else:
            cs = float(self.get_ngram_count(history)) # sum_w(c(history+w)) = c(history)
        if cs:
            D = self.Ds[n-1]
            N1rx = self.N1rxs[n-1].get(tuple(history), 0)
            l1 = D / float(cs) * N1rx      # 1 - gamma
            history_key = " ".join(history + [""])
            counts = self.ngrams[n-1]

            for i in xrange(len(words)):
                c = counts.get(history_key + words[i], 0)  # get_ngram_count
                a  = c - D
                if a < 0: a = 0
                a /= cs  # alpha
                ps[i] = a + l1 * ps[i]

        return ps

    def kneser_ney_i_numpy(self, history_indices, word_indices):
        """ kneser_ney interpolated, iterative, vectorized with numpy """

        n = len(history_indices) + 1

        # order 0
        vp = empty(len(word_indices))
        vp.fill(1.0/self.num_words) # uniform distribution

        # order 1..n-1
        for j in range(0, n-1):
            h = history_indices[n-j-1:]
            hnode = self.ngrams.get_node(h)
            if hnode:
                N1pxrx = float(hnode.N1pxrx) # number of permutations around r
                if N1pxrx:
                    D = self.Ds[j]
                    N1prx = len(hnode.children) # number of word types following the history

                    # number of word types seen to precede ngram
                    if len(h) == 0:
                        # number of children >= number of searched words
                        # -> shortcut for root, all unigrams exist
                        children = self.ngrams.children
                        vc = array([children[wi].N1pxr for wi in word_indices])
                    else:
                        # number of children << number of searched words
                        # everything from bigrams very likely has only a few children
                        vc = zeros(len(word_indices))
                        for child in hnode.children:
                            index = binsearch(word_indices, child.wi) # word_indices have to be sorted by index
                            if index != -1:
                                vc[index] = child.N1pxr

                    va = vc - D
                    va.clip(0, inf, va)
                    va /= N1pxrx
                    vp *= D / float(N1pxrx) * N1prx     # 1 - gamma
                    vp += va

        # order n
        hnode = self.ngrams.get_node(history_indices)
        if hnode:
            if n == 1:
                cs = float(self.num_tokens)
            else:
                cs = float(self.ngrams.get_count(history_indices)) # sum_w(c(history+w)) = c(history)
            if cs:
                D = self.Ds[n-1]
                N1prx = len(hnode.children)   # number of word types following the history

                vc = zeros(len(word_indices))
                for child in hnode.children:
                    index = binsearch(word_indices, child.wi) # word_indices have to be sorted by index
                    if index > 0:
                        vc[index] = child.count

                va = vc - D
                va.clip(0, inf, va)
                vp *= D / float(cs) * N1prx      # 1 - gamma
                va /= cs
                vp += va

        return vp

    def split_context(self, context):
        n   = min(self.max_order, len(context))
        history = context[-n:-1]
        prefix  = context[-1]
        return history, prefix

    def predict(self, context):
        history, prefix = self.split_context(context)
        choices = []
        for w in self.get_vocabulary():
            if w.startswith(prefix):  # word completion
                ngram = history + [w]
                p = self.get_probability(ngram)
                choices.append([w, p])

        choices.sort(key=lambda x: x[1], reverse=True)

        return choices

    def predict_kn(self, context):
        history, prefix = self.split_context(context)
        choices = []
        ngram = history + [None]
        for w in self.get_vocabulary():
            if w.startswith(prefix):  # word completion
                ngram[-1] = w
                p = self.kneser_ney_i_i(ngram)
                choices.append([w, p])

        choices.sort(key=lambda x: x[1], reverse=True)

        return choices

    def predict_kn_v(self, context):
        history, prefix = self.split_context(context)

        if prefix:
            words = [w for w in self.get_vocabulary() if w.startswith(prefix)]
        else:
            words = self.get_vocabulary()

        ps = self.kneser_ney_i_v(history, words)

        ai = range(len(words))
        ai.sort(key=lambda i: ps[i], reverse=True)
        choices = [(words[i], ps[i]) for i in ai[:50]]

        return choices

    def predict_kn_numpy_chunk(self, context, k=100):
        history, prefix = self.split_context(context)

        words = [w for w in self.get_vocabulary() if w.startswith(prefix)]
        k = 100
        choices = []
        for i in xrange(0,len(words),k):
            chunk = words[i:i+k]
            ps = self.kneser_ney_i_numpy(history, chunk)
            choices.extend(zip(chunk, ps))

        choices.sort(key=lambda x: x[1], reverse=True)

        return choices

    def predict_kn_numpy(self, context):
        history, prefix = self.split_context(context)

        words = [w for w in self.get_vocabulary() if w.startswith(prefix)]
        vp = self.kneser_ney_i_numpy(history, words)
        choices = zip(words, vp)

        choices.sort(key=lambda x: x[1], reverse=True)

        return choices

    def predict_kn_numpy(self, context):
        history, prefix = self.split_context(context)

        history_indices = self.ngram_to_indices(history)

        if prefix:
            word_indices = [i for i,w in enumerate(self.vocabulary.words) if w.startswith(prefix)]
        else:
            word_indices = range(len(self.vocabulary))

        vp = self.kneser_ney_i_numpy(history_indices, word_indices)
        return [(self.vocabulary.words[word_indices[i]], vp[i]) for i in vp.argsort()[:-(50+1):-1]]
        #return [(self.vocabulary.words[word_indices[i]], vp[i]) for i in vp.argsort()[::-1]]

    def print_choices(self, context, choices):
        history, prefix = self.split_context(context)
        print "history:", history, "prefix '%s' " % prefix
        print "smoothing: " + self.smooth_name[self.smoothing]
        psum = 0
        counts = []
        for x in choices:
            ngram = history + [x[0]]
            psum += x[1]
            padding = max(self.max_order-len(context),0)
            ng = [""]*padding + ngram
            counts.append([self.get_ngram_count(ng[i:]) for i in range(self.max_order)])

        print "Probability sum %f for %d results" % (psum,len(choices))   # ought to be 1.0 for the whole vocabulary
        for i,x in enumerate(choices[:20]):
            print "%10f " % x[1] + "".join("%8d " % c for c in counts[i]) + "'%s'" % x[0]

if __name__ == '__main__':
    main()

